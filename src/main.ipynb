{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# read data from tables\n",
    "apis_df = pd.read_csv('../datasets/apis.csv', usecols = [0, 1, 2, 3, 5])\n",
    "mashups_df = pd.read_csv('../datasets/mashups.csv', usecols = [0, 1, 2, 3, 4])\n",
    "\n",
    "# drop rows that contain null values\n",
    "apis_df.dropna(inplace = True)\n",
    "mashups_df.dropna(inplace = True)\n",
    "\n",
    "# correct index numbers\n",
    "apis_df.reset_index(drop = True, inplace = True)\n",
    "mashups_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text filter list of English stopwords and special characters\n",
    "text_filter = list(stopwords.words(\"english\"))\n",
    "special_characters = [',', '/', '-', '.', ';']\n",
    "for char in special_characters:\n",
    "  text_filter.append(char)\n",
    "\n",
    "# initialize text stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# tokenize, apply filter to, and stem text\n",
    "def text_filter_function(text):\n",
    "  result = ''\n",
    "  tokens = word_tokenize(str(text))\n",
    "  for token in tokens:\n",
    "    if token not in text_filter:\n",
    "      result += porter.stem(token.lower())\n",
    "      result += ' '\n",
    "  return result\n",
    "\n",
    "# process the description column to create new bag of words column\n",
    "apis_df['description_words'] = apis_df['categories'] + ' ' + apis_df['description']\n",
    "apis_df['description_words'] = apis_df['description_words'].apply(text_filter_function)\n",
    "mashups_df['description_words'] = mashups_df['tagList'] + ' ' + mashups_df['description']\n",
    "mashups_df['description_words'] = mashups_df['description_words'].apply(text_filter_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I want some APIs with map, music and social media sharing function'\n",
    "\n",
    "# add a new row to the dfs\n",
    "new_mashups_df = mashups_df.append({ 'description_words': text_filter_function(query) }, ignore_index = True)\n",
    "query_mashup_id = new_mashups_df.shape[0] - 1\n",
    "\n",
    "new_apis_df = apis_df.append({ 'description_words': text_filter_function(query) }, ignore_index = True)\n",
    "query_api_id = new_apis_df.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mashup</th>\n",
       "      <th>apiList</th>\n",
       "      <th>tagList</th>\n",
       "      <th>description</th>\n",
       "      <th>description_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5391</th>\n",
       "      <td>7860</td>\n",
       "      <td>Xbox Radar</td>\n",
       "      <td>18233;289;2;11</td>\n",
       "      <td>eCommerce;Games</td>\n",
       "      <td>Xbox Radar helps people find Xbox 360 products...</td>\n",
       "      <td>ecommerc game xbox radar help peopl find xbox ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5392</th>\n",
       "      <td>7861</td>\n",
       "      <td>America 24 7</td>\n",
       "      <td>4</td>\n",
       "      <td>Mapping;Photos</td>\n",
       "      <td>MSN Virtual Earth used to plot photos from the...</td>\n",
       "      <td>map photo msn virtual earth use plot photo hre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>7862</td>\n",
       "      <td>LinkPut</td>\n",
       "      <td>18201</td>\n",
       "      <td>Search;Wiki</td>\n",
       "      <td>LinkPut is an experiment that attempts to crea...</td>\n",
       "      <td>search wiki linkput experi attempt creat bette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>7867</td>\n",
       "      <td>HotOrNot Google Maps</td>\n",
       "      <td>18305</td>\n",
       "      <td>Mapping;Dating</td>\n",
       "      <td>Find people to meet on HotOrNot.com by browsin...</td>\n",
       "      <td>map date find peopl meet hotornot.com brows go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>7869</td>\n",
       "      <td>flickr graph</td>\n",
       "      <td>1</td>\n",
       "      <td>Photos;Visualizations</td>\n",
       "      <td>Social network visualization using Flickr API.</td>\n",
       "      <td>photo visual social network visual use flickr ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                mashup         apiList                tagList  \\\n",
       "5391  7860            Xbox Radar  18233;289;2;11        eCommerce;Games   \n",
       "5392  7861          America 24 7               4         Mapping;Photos   \n",
       "5393  7862               LinkPut           18201            Search;Wiki   \n",
       "5394  7867  HotOrNot Google Maps           18305         Mapping;Dating   \n",
       "5395  7869          flickr graph               1  Photos;Visualizations   \n",
       "\n",
       "                                            description  \\\n",
       "5391  Xbox Radar helps people find Xbox 360 products...   \n",
       "5392  MSN Virtual Earth used to plot photos from the...   \n",
       "5393  LinkPut is an experiment that attempts to crea...   \n",
       "5394  Find people to meet on HotOrNot.com by browsin...   \n",
       "5395     Social network visualization using Flickr API.   \n",
       "\n",
       "                                      description_words  \n",
       "5391  ecommerc game xbox radar help peopl find xbox ...  \n",
       "5392  map photo msn virtual earth use plot photo hre...  \n",
       "5393  search wiki linkput experi attempt creat bette...  \n",
       "5394  map date find peopl meet hotornot.com brows go...  \n",
       "5395  photo visual social network visual use flickr ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mashups_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(analyzer = str.split, max_features = 10000)\n",
    "\n",
    "mashup_description_matrix = tf_idf.fit_transform(mashups_df['description_words']).toarray()\n",
    "mashup_cos_sim_matrix = cosine_similarity(mashup_description_matrix, mashup_description_matrix)\n",
    "\n",
    "api_description_matrix = tf_idf.fit_transform(apis_df['description_words']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return APIs with top scores in the top k related mashups of the query\n",
    "def api_scores_for_top_k_mashups(mashup_id, k = 15):\n",
    "  # sort the top related mashups descending using cosine similarity score, and pick the first k elements\n",
    "  score_series = pd.Series(mashup_cos_sim_matrix[mashup_id]).sort_values(ascending = False).iloc[1:k+1]\n",
    "\n",
    "  apis_in_top_k_mashups = []\n",
    "  for i in range(k):\n",
    "    # retrieve a list of API id's from mashups_df\n",
    "    api_list = mashups_df.iloc[score_series.index[i]]['apiList'].split(';')\n",
    "    api_list = [int(api) for api in api_list]\n",
    "    # append the list of API id's\n",
    "    apis_in_top_k_mashups.append(api_list)\n",
    "\n",
    "  # create a list of unrepeated API id's\n",
    "  all_apis = []\n",
    "  for i in range(k):\n",
    "    for api in apis_in_top_k_mashups[i]:\n",
    "      if api not in all_apis:\n",
    "        all_apis.append(api)\n",
    "\n",
    "  # for each API, if it is used in one of the top k related mashups, add its count by 1\n",
    "  count = {}\n",
    "  for i in range(len(all_apis)):\n",
    "    count[all_apis[i]] = 0\n",
    "    for j in range(k):\n",
    "      if all_apis[i] in apis_in_top_k_mashups[j]:\n",
    "        count[all_apis[i]] += 1\n",
    "\n",
    "  # each element in api_scores will be API id and its ranking score (its count divided by k times 100)\n",
    "  api_scores = []\n",
    "  for i in range(len(all_apis)):\n",
    "    api_scores.append([all_apis[i], count[all_apis[i]]])\n",
    "\n",
    "  # sort api_scores by ranking score from highest to lowest\n",
    "  api_scores = sorted(api_scores, reverse = True, key = lambda x: x[1])[:10]\n",
    "\n",
    "  # return a dictionary of API's name, descrition and url\n",
    "  recommendations = []\n",
    "  for i in range(len(api_scores)):\n",
    "    api = apis_df.loc[apis_df['id'] == api_scores[i][0]].values[0]\n",
    "    recommendations.append({ 'name': api[1], 'description': api[3], 'url': api[4], 'count': api_scores[i][1] })\n",
    "\n",
    "  return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon Product Advertising API'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apis_df.loc[apis_df['id'] == 2].values[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'OpenStack Identity API',\n",
       "  'description': 'OpenStack is a provider of various open source project components that facilitate the establishment of cloud services. It has provided standard APIs that are well suited for multiple implementations. The OpenStack Identity API supports the validation of user access credentials. The API supplies the authentication tokens that users must furnish prior to gaining access permissions for OpenStack APIs and services. The API transmits HTTP-formatted requests and responses.',\n",
       "  'url': 'openstack-identity',\n",
       "  'count': 0.3872930461889199},\n",
       " {'name': 'hon.jp API',\n",
       "  'description': 'Requires Japanese-enabled PC. This API is provided by Japanese eBook search site HON.JP. Kanji-enabled users can access our proprietary Japanese ebook metadata database, covering titles sold in PC, WAP phones, eBook readers, etc. Upgraded to ver.2.0 on 2007-11-01 and improved on reponsiveness along with new methods such as  sort ,  page  and  mode .',\n",
       "  'url': 'honjp',\n",
       "  'count': 0.33786028286411934},\n",
       " {'name': 'Embarke API',\n",
       "  'description': 'Embarke provides developer tools for social communications, delivering a back-end conversation platform capable of powering multiple communication platforms, allowing companies to focus on their products. The Embarke REST API exposes six resources: users, networks, accounts, conversations, messages, and contacts.',\n",
       "  'url': 'embarke',\n",
       "  'count': 0.3240450907892455},\n",
       " {'name': 'The Society of Authors Member Search API',\n",
       "  'description': 'The Society of Authors, based in the UK, has been serving the interests of professional writers for more than a century. Today it has more than 9,000 members and associates working as novelists, textbook writers, illustrators, ghost writers, translators, and more. Authors become eligible to join as soon as they are offered a contract.  One of the site s services is the Member Search, which allows visitors to search for writer or translator who is part of the Society. This service is also available programmatically as a SOAP-based API.',\n",
       "  'url': 'society-authors-member-search',\n",
       "  'count': 0.3155717492082357},\n",
       " {'name': 'Lonely Planet API',\n",
       "  'description': 'LonelyPlanet, a leading travel publisher, offers an API to integrate guidebook information into web applications, widgets, and mobile phone interfaces.  The 500 LonelyPlanet titles cover most of the world s travel destinations.  The API includes PHP and mobile examples, and is currently in closed beta.',\n",
       "  'url': 'lonely-planet',\n",
       "  'count': 0.3122373518162693},\n",
       " {'name': 'Hexillion Whois API',\n",
       "  'description': 'The Hexillion Whois API provides a single, consistent, programmable interface to the Whois system. It automatically queries the correct Whois server, breaks the Whois record details out into separate fields, returns XML, and is built for volume use.',\n",
       "  'url': 'hexillion-whois',\n",
       "  'count': 0.3107215196909979},\n",
       " {'name': 'Tweet Press API',\n",
       "  'description': 'Tweet Press is a free blogging service that lets users post updates longer than 140 characters to Twitter, organize them on their own personal blog by topic, and gather all replies and comments in one place. The Tweet Press REST API allows users to create a new Tweet Press Posting, and corresponding Tweet, using HTTP requests. The API uses HTTP POST calls and responses are formatted in XML.',\n",
       "  'url': 'tweet-press',\n",
       "  'count': 0.30906418454965684},\n",
       " {'name': 'National Library of Medicine ChemSpell API',\n",
       "  'description': 'The National Library of Medicine (NLM) is the world s largest medical library. The Library collects materials and provides information and research services in all areas of biomedicine and health care. The ChemSpell Web Service API provides chemical name spell checking and chemical name synonym look-up. ChemSpell contains more than 1.3 million chemical names related to organic, inorganic, pharmaceutical, toxicological, and environmental health topics. Developers can use the API to write applications that connect remotely to the ChemSpell Web service. The API uses SOAP calls and responses are formatted in XML.',\n",
       "  'url': 'national-library-medicine-chemspell',\n",
       "  'count': 0.3047792354665225},\n",
       " {'name': 'Komoju API',\n",
       "  'description': 'Komoju is a Japanese company focused on payments, eCommerce, and banking services. The API supports all payment methods in Japanese Yen(JPY), offering REST protocol and a hosted page for integrations. Developers can authenticate with HTTP basic auth to send requests and receive responses in JSON format.',\n",
       "  'url': 'komoju',\n",
       "  'count': 0.30339229133043644},\n",
       " {'name': 'Yammer Data Export API',\n",
       "  'description': 'The Yammer Data Export API allows developers  verified Admin only  to package and export all messages, Notes, Files, topics, users, and groups. This API also allows for performing a one-time export simply by specifying the starting date and ending date which they would like to export data. Yammer provides social and collaboration software for businesses and enterprises of all sizes.',\n",
       "  'url': 'yammer-data-export',\n",
       "  'count': 0.3029969629389503},\n",
       " {'name': 'Google Cloud Data Loss Prevention API',\n",
       "  'description': 'The Google Cloud Data Loss Prevention API offers scalable classification and redaction services for sensitive information such as credit card numbers, social security numbers, and passport numbers. The API allows developers to manage, analyse, protect, and report small text streams, and large datasets. 40 patterns and detectors are used to classify sensitive information. Currently, the Google Cloud Data Loss Prevention API is in beta.',\n",
       "  'url': 'google-cloud-data-loss-prevention',\n",
       "  'count': 0.3001235004046661},\n",
       " {'name': 'TripCheck API',\n",
       "  'description': 'TripCheck is a service provided by the Oregon Department of Transportation and provides users with access to weather, trip planning assistance, and traffic information. The TripCheck API makes parking data from the Portland International Airport available, as well as traffic incident data, road weather reports, automated roadside weather station data, roadway sensor data, dynamic message sign data, highway weight restrictions, links to roadside camera images. Data is available in XML. An account is required with service.',\n",
       "  'url': 'tripcheck',\n",
       "  'count': 0.29383637635425414},\n",
       " {'name': 'Utrace API',\n",
       "  'description': 'Utrace is an online service that provides users with information regarding the location of IP addresses and domain names. The Utrace API lets developers integrate its services into their applications, enabling their consumers to access data from Utrace directly.',\n",
       "  'url': 'utrace',\n",
       "  'count': 0.28777531845348053},\n",
       " {'name': 'Adform Orders API',\n",
       "  'description': 'The Adform Orders API integrates buyer orders into web services. Developers can include name, start date, end date, budget, campaign ID, and active status. This API returns JSON formatted responses.',\n",
       "  'url': 'adform-orders',\n",
       "  'count': 0.2816430643231967},\n",
       " {'name': 'PHL API',\n",
       "  'description': 'PHL is the API of Philadelphia Geo Data. Determine zones, districts, and wards based on a street address. View some datasets on a map. The API is powered by GeoCouch, is RESTful and can return responses in JSON.',\n",
       "  'url': 'phl',\n",
       "  'count': 0.2805861755935451}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommend the top k related APIs to the query\n",
    "def api_scores_for_top_k_apis(api_id, k = 15):\n",
    "  api_scores = []\n",
    "\n",
    "  # for all API entries except the last one (which is our query)\n",
    "  for i in range(apis_df.shape[0] - 1):\n",
    "    # calculate the cosine similarity between all mashups and the query\n",
    "    cos_sim_score_i = cosine_similarity([api_description_matrix[api_id]], [api_description_matrix[i]])[0][0]\n",
    "    # append index and related cosine similarity score\n",
    "    api_scores.append([i, cos_sim_score_i])\n",
    "\n",
    "  # sort the array descending using cosine similarity score, and pick the first k elements\n",
    "  api_scores = sorted(api_scores, reverse = True, key = lambda x: x[1])[:k]\n",
    "\n",
    "  recommendations = []\n",
    "  for i in range(len(api_scores)):\n",
    "    # retrieve API's name from apis_df\n",
    "    api = apis_df.loc[apis_df['id'] == api_scores[i][0]].values[0]\n",
    "    # append a tuple of API name and its cosine similarity score\n",
    "    recommendations.append({ 'name': api[1], 'description': api[3], 'url': api[4], 'count': api_scores[i][1] })\n",
    "\n",
    "  return recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
